# -*- coding: utf-8 -*-
"""
Created on Thu Feb 25 20:59:16 2021

@author: owner
"""

# library for machine learning algorithms
import sklearn.ensemble as ske

# for numerical-related activities
import numpy as nm  

# used for importing/managing the dataset
import pandas as pd  

#this is for the accuracy graph of results
import matplotlib.pyplot as mtp  

# read the dataset on malware (the .csv file containing info on android malware) --> the result is a dataframe object
data_set = pd.read_csv('./MyData/testing.csv' , encoding = "ISO-8859-1")

# splitting the dataset into the features(x) and outcomes (y)
# the below commands will isolate the columns for each split and drop the useless ones --> axis=1 specifies to look at the columns instead of rows (axis=0)
x = data_set.drop( ["apk_file_name"], axis=1 ).values
y = data_set["is_malware"].values



x = pd.DataFrame(x).fillna(value=0) #replace nulls with 0 so as to not trigger any errors

#print("testing this ... :  %i" % x.shape[1]) #thisis for the dimensions of the array, this case prints the number of columns (use 0 for number of rows)
#print(nm.where(nm.isnan(x))) #print the row/colmn index where value is null

"""
# scale the feature's values down so that they wont cause out of bounds errors, etc..  
from sklearn.preprocessing import StandardScaler    
scalr = StandardScaler()    
x_train = scalr.fit_transform(x_train)    
x_test = scalr.transform(x_test)   
"""

print("Total Feature Counter: %i" % x.shape[1])


important_features = ske.ExtraTreesClassifier().fit(x,y)
# SelectFromModel can be used with any model AND IT RETURNS THE IMPORTANT FEATURES BASED ON WEIGHT (caluculated through averages and a threshold boundary) VALUES (!!OF THAT MODEL YOU PASSED) --> the features that have little effect in influencing the result will be dropped --> now fit the training set to this model to change the training set to the model's specifications (in this case, removing the unimportant features)          
from sklearn.feature_selection import SelectFromModel
feature_model = SelectFromModel(important_features, prefit = True)
x_new = feature_model.transform(x)


print("Important Feature Counter: %i" % x_new.shape[1])


# Splitting the dataset into training and test set.  
from sklearn.model_selection import train_test_split  
x_train, x_test, y_train, y_test= train_test_split(x_new, y, test_size= 0.25, random_state=42)

'''
#feature Scaling  --> if you let the values get too high, the graph will take too much memory (in gigabytes even!)
from sklearn.preprocessing import StandardScaler    
st_x= StandardScaler()    
x_train= st_x.fit_transform(x_train)    
x_test= st_x.transform(x_test)  
'''

# sort the important_features based on their ".feature_importances_" aka the array of ordered features (based on weight values) and only include the ones selected and paced in the x_new array (which are the top x important ones)
sorted_features = sorted(nm.argsort(important_features.feature_importances_))[::-1][:x_new.shape[1]] 

# this will be our final collection of important features (populated in the below for loop)
final_features_model = []

for i in sorted_features:
    final_features_model.append(data_set.columns[1+i]) #we are adding a 1 since we took away the first  column from the dataset when preparing our data and now we are referencing the dataset 
    
    
    
# this is an array of diffirent classifiers --> used in this case for ensemble learning    

from sklearn import tree
from sklearn.naive_bayes import GaussianNB  

classifier_alogorithms = {
    "ADABoost": ske.AdaBoostClassifier(n_estimators=100),
    "Bayes": GaussianNB(),
    "DecisionTree": tree.DecisionTreeClassifier(max_depth=10),
    "GradientBoost": ske.GradientBoostingClassifier(n_estimators=100),
    "RandomForest": ske.RandomForestClassifier(n_estimators=50)
    }


ensemble_results = {}

for i in classifier_alogorithms:
    my_classifier = classifier_alogorithms[i]
    my_classifier.fit(x_train, y_train)
    score = my_classifier.score(x_test, y_test)
    print("Classifier Algorithm Used is: %s  --> its accuracy is: %f" % (i, score*100) )
    ensemble_results[i] = score
    



top_classifier_algorithm_name = max(ensemble_results, key=ensemble_results.get)

print("top classifier algorithm is: %s" % top_classifier_algorithm_name)


top_classifier_algorithm = classifier_alogorithms[top_classifier_algorithm_name]




import pickle
filename = './MyData/trained_model.sav'
pickle.dump(top_classifier_algorithm, open(filename, 'wb'))

#to extract important features
filename = './MyData/key_feature_model.sav'
pickle.dump(feature_model, open(filename, 'wb'))






"""

final_features_model
"""



